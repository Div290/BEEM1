# ğŸŒŸ BEEM: Boosting Performance of Early Exit DNNs using Multiple Exit Classifiers as Experts  

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-1.4.0-red?style=flat-square&logo=pytorch" />
  <img src="https://img.shields.io/badge/Transformers-2.5.1-blue?style=flat-square&logo=huggingface" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=flat-square" />
  <img src="https://img.shields.io/github/issues/your-repo/BEEM?style=flat-square" />
</p>  

ğŸš€ **BEEM** introduces a cutting-edge **Early Exit strategy** that leverages **multiple exit classifiers as experts**, improving inference efficiency while preserving model accuracy. This approach significantly reduces computational costs, making DNNs faster and more adaptive across various tasks.  

---

## ğŸ“Œ Table of Contents  
- [âœ¨ Introduction](#-introduction)  
- [âš¡ Key Features](#-key-features)  
- [ğŸ›  Installation & Requirements](#-installation--requirements)  
- [ğŸ“– Training](#-training)  
- [ğŸ” Inference](#-inference)  
- [ğŸ“Š Datasets](#-datasets)  
- [ğŸ¤ Acknowledgements](#-acknowledgements)  

---

## âœ¨ Introduction  
Deep Neural Networks (DNNs) are powerful but computationally expensive. **BEEM** addresses this challenge by implementing an **expert-based early exit mechanism**, allowing intermediate layers to make confident predictions, thus reducing latency and computation.  

ğŸ”¹ **Why BEEM?**  
âœ” **Faster inference** with minimal accuracy drop.  
âœ” **Adaptive early exits** based on sample complexity.  
âœ” **Expert-based classification** for improved decision-making.  
âœ” **Optimized for resource-constrained environments.**  

---

## âš¡ Key Features  
âœ… **Efficient Early Exit Strategy** â€“ Speeds up inference with **minimal accuracy loss**.  
âœ… **Multi-Exit Classifiers as Experts** â€“ Enhances performance through **specialized classifiers** at different depths.  
âœ… **Domain Adaptability** â€“ Works across **text, vision, and multimodal tasks**.  
âœ… **Built on Hugging Face Transformers** â€“ Easily extendable with **pre-trained language models**.  

---

## ğŸ›  Installation & Requirements  
To get started, install the required dependencies:  

```bash
pip install torch==1.4.0 transformers==2.5.1
```

## ğŸ“– Training
Fine-tune a pre-trained language model and train the internal classifiers using:

```bash
bash (al)bert_finetuning.sh
```

## ğŸ” Inference
Run inference with trained classifiers using:

```bash
bash expert_infer_(al)bert.sh
```


## ğŸ“Š Datasets
We use GLUE benchmark datasets, available at [GLUE Datasets](https://gluebenchmark.com/).

## ğŸ¤ Acknowledgements
This work builds upon the Hugging Face Transformers library. Special thanks to the PABEE authors for their contributions to early exit methods.

## ğŸ¯ Contribute & Stay Updated!
ğŸ“Œ Found a bug or have a suggestion? Open an [issue](https://github.com/your-repo/BEEM/issues).

ğŸ“Œ Welcome to contribute!.

ğŸ“Œ Star â­ this repo if you find it useful!

<p align="center"> <b>ğŸš€ Let's make DNN inference faster and smarter together!</b> </p> 
